{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NPCA-TEAM/COVID-19/blob/main/Scripts/%204%20-%20TREINAR_MODELOS_covid_casos_ver1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDVRez9ivbXr"
      },
      "source": [
        "#Instalação, importação de bibliotecas e inicialização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvpQPdBdUian"
      },
      "source": [
        "##Instalação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfOWwR3W8efr"
      },
      "outputs": [],
      "source": [
        "#Inicia permitindo acesso ao GDrive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEt-ieggipCk"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5N9AQ0SitfJ"
      },
      "outputs": [],
      "source": [
        "!pip install darts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHVSe41RitIR"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib==3.1.3  #é necessário restart runtime "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTTMJo3oexqp"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3kwbr4cUrHH"
      },
      "source": [
        "##Importação das Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvcVpnCPjefR"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import DateFormatter\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from darts import timeseries\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from darts import concatenate\n",
        "\n",
        "from darts.models import (    \n",
        "    NBEATSModel,\n",
        "    TCNModel,\n",
        "    TransformerModel,\n",
        "    TFTModel,\n",
        "    NHiTSModel #NHiTS\n",
        "    )\n",
        "\n",
        "from darts.metrics import mape, rmse, r2_score\n",
        "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
        "from darts.utils.missing_values import fill_missing_values\n",
        "from darts.models import KalmanFilter\n",
        "\n",
        "from darts.utils.likelihood_models import (\n",
        "    GaussianLikelihood, \n",
        "    QuantileRegression \n",
        "    )\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import logging\n",
        "logging.disable(logging.CRITICAL)\n",
        "torch.manual_seed(1); np.random.seed(1)  # for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiNGBVhbu12n"
      },
      "source": [
        "#Dados e campos de entrada do framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJPUoQRfjuXJ"
      },
      "outputs": [],
      "source": [
        "#Campos de entrada do framework\n",
        "NAME_SERIE_SYMPTOM_CASES = \"Casos_DataSintoma_MM_atual_PA\"   #Variavel onde será adicionada o nome para a serie a ser analisada\n",
        "NAME_SERIE_PUBLICATION_CASES = \"Casos_Publicacao_MM_atual_PA\"   #Variavel onde será adicionada o nome para a serie a ser analisada\n",
        "\n",
        "#Número de anos para validação/teste. Similar a quantidade de dados de teste\n",
        "#Se num_days_of_slice_val = 1, indica que o ultimo ano da serie será usado para testar o modelo\n",
        "#Também indica a fatia usada como validação na fase de treinamento\n",
        "VALIDATION_DAYS_SLICED = 7\n",
        "\n",
        "#Número que indica o tamanho da fatia usada como treino na fase de treinamento. \n",
        "#Logo, o processo disposto neste script é baseado no metodo de janela deslizante.\n",
        "WORKOUT_DAYS_SLICED = 30\n",
        "\n",
        "#Numero de passos da previsão a frente da serie\n",
        "FORECAST_DAYS = 7\n",
        "\n",
        "# Arquivo com a base de dados\n",
        "PATH_DATASET = '/content/drive/MyDrive/NPCA - COVID/_CASOS/DataSet/' \n",
        "\n",
        "#Pasta para armazenamento do modelo\n",
        "PATH_MODELS = '/content/drive/MyDrive/NPCA - COVID/_CASOS/Models/'\n",
        "PATH_PARAMS = '/content/drive/MyDrive/NPCA - COVID/_CASOS/BestModelParameters/'\n",
        "PATH_FIGS = '/content/drive/MyDrive/NPCA - COVID/_CASOS/Figs/'\n",
        "PATH_CSV = '/content/drive/MyDrive/NPCA - COVID/_CASOS/ForecastsOutputModel/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xX7CbOt0G9_"
      },
      "source": [
        "#Leitura do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvP4WKsXj6BV"
      },
      "outputs": [],
      "source": [
        "#Seleciona o arquivo da base\n",
        "FILE_LIST = glob.glob(PATH_DATASET + '*')\n",
        "\n",
        "#Seleciona o ultimo arquivo que foi criado e adiciona na pasta\n",
        "FILE_PATH = max(FILE_LIST, key=os.path.getctime)\n",
        "\n",
        "#Faz leitura do arquivo excel como datafrade\n",
        "DATAFRAME = pd.read_excel(FILE_PATH, sheet_name=0)\n",
        "\n",
        "DATAFRAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlp3EKa5PTP6"
      },
      "source": [
        "#Seleção das variáveis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9haIlfYQhfP"
      },
      "outputs": [],
      "source": [
        "#Seleção das variáveis alvo de casos\n",
        "SYMPTOM_CASES = DATAFRAME.loc[:, ['data', 'Casos_DataSintoma_MM_atual_PA']]\n",
        "PUBLICATION_CASES = DATAFRAME.loc[:, ['data', 'Casos_Publicacao_MM_atual_PA']]\n",
        "\n",
        "#Seleção das variáveis auxiliares\n",
        "#SYMPTOM_CASES_7DAYS = DATAFRAME.loc[:, ['data', 'Casos_DataSintoma_MM_7dias_PA']].fillna(0) \n",
        "#PUBLICATION_CASES_7DAYS = DATAFRAME.loc[:, ['data', 'Casos_Publicacao_MM_7dias_PA']].fillna(0)\n",
        "SYMPTOM_CASES_14DAYS = DATAFRAME.loc[:, ['data', 'Casos_DataSintoma_MM_14dias_PA']].fillna(0)\n",
        "PUBLICATION_CASES_14DAYS = DATAFRAME.loc[:, ['data', 'Casos_Publicacao_MM_14dias_PA']].fillna(0)\n",
        "#OBS: No treinamento usaremos somete o 14Dias. \n",
        "\n",
        "VACCINATION_1DOSE = DATAFRAME.loc[:, ['data', 'Vacinacao_Dose1_%decimal_PA']].fillna(0)\n",
        "VACCINATION_2DOSE = DATAFRAME.loc[:, ['data', 'Vacinacao_Dose2_%decimal_PA']].fillna(0)\n",
        "VACCINATION_3DOSE = DATAFRAME.loc[:, ['data', 'Vacinacao_Dose3_%decimal_PA']].fillna(0)\n",
        "\n",
        "#Renomeia variaveis.\n",
        "SYMPTOM_CASES_7PREVIOUSDAYS = SYMPTOM_CASES_14DAYS.rename(columns = {'Casos_DataSintoma_MM_14dias_PA':'Casos_DataSintoma_MM_7diasAnteriores_PA'}, inplace = False)\n",
        "PUBLICATION_CASES_7PRECIOUSDAYS = PUBLICATION_CASES_14DAYS.rename(columns = {'Casos_Publicacao_MM_14dias_PA':'Casos_Publicacao_MM_7diasAnteriores_PA'}, inplace = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCSPZIK4DriJ"
      },
      "source": [
        "#Instanciação das séries temporais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT7WRvs4DvHa"
      },
      "outputs": [],
      "source": [
        "# criação das séries temporais alvos\n",
        "SERIE_SYMPTOM_CASES = timeseries.TimeSeries.from_dataframe(df=SYMPTOM_CASES, time_col='data')\n",
        "SERIE_PUBLICATION_CASES = timeseries.TimeSeries.from_dataframe(df=PUBLICATION_CASES, time_col='data')\n",
        "\n",
        "plt.figure(figsize = (10, 6))\n",
        "\n",
        "SERIE_SYMPTOM_CASES.plot()\n",
        "SERIE_PUBLICATION_CASES.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BegUBtjt7As0"
      },
      "outputs": [],
      "source": [
        "# criação das séries temporais auxiliares de casos\n",
        "#SERIE_SYMPTOM_CASES_7DAYS = timeseries.TimeSeries.from_dataframe(df=SINTOMA_7DIAS, time_col='data')\n",
        "#SERIE_PUBLICATION_CASES_7DAYS = timeseries.TimeSeries.from_dataframe(df=PUBLICACAO_7DIAS, time_col='data')\n",
        "SERIE_SYMPTOM_CASES_7PREVIOUSDAYS = timeseries.TimeSeries.from_dataframe(df=SYMPTOM_CASES_7PREVIOUSDAYS, time_col='data')\n",
        "SERIE_PUBLICATION_CASES_7PREVIOUSDAYS = timeseries.TimeSeries.from_dataframe(df=PUBLICATION_CASES_7PRECIOUSDAYS, time_col='data')\n",
        "\n",
        "plt.figure(figsize = (10, 6))\n",
        "\n",
        "#SERIE_SYMPTOM_CASES_7DAYS.plot()\n",
        "#SERIE_PUBLICATION_CASES_7DAYS.plot()\n",
        "SERIE_SYMPTOM_CASES_7PREVIOUSDAYS.plot()\n",
        "SERIE_PUBLICATION_CASES_7PREVIOUSDAYS.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbxhoAPE7lQO"
      },
      "outputs": [],
      "source": [
        "# criação das séries temporais auxiliares de vacinação\n",
        "serie_vaccination_1Dose = timeseries.TimeSeries.from_dataframe(df=VACCINATION_1DOSE, time_col='data')\n",
        "serie_vaccination_2Dose = timeseries.TimeSeries.from_dataframe(df=VACCINATION_2DOSE, time_col='data')\n",
        "serie_vaccination_3Dose = timeseries.TimeSeries.from_dataframe(df=VACCINATION_3DOSE, time_col='data')\n",
        "\n",
        "plt.figure(figsize = (10, 6))\n",
        "\n",
        "serie_vaccination_1Dose.plot()\n",
        "serie_vaccination_2Dose.plot()\n",
        "serie_vaccination_3Dose.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZlwXrseqluW"
      },
      "source": [
        "#Filtro de suaviação das variáveis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6oFZE73qiau"
      },
      "outputs": [],
      "source": [
        "def filter_for_suavization(serie):\n",
        "  filterKalman = KalmanFilter(dim_x = 1)\n",
        "  filterKalman.fit(serie)\n",
        "  serieTemp_filtered = filterKalman.filter(serie)\n",
        "\n",
        "  return serieTemp_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GGv0OcyFRFj"
      },
      "outputs": [],
      "source": [
        "#Suavização das series alvos\n",
        "serie_symptom_cases_smoothed = filter_for_suavization(SERIE_SYMPTOM_CASES)\n",
        "serie_publication_cases_smoothed = filter_for_suavization(SERIE_PUBLICATION_CASES)\n",
        "\n",
        "plt.figure(figsize = (10, 6))\n",
        "\n",
        "SERIE_SYMPTOM_CASES.plot(label=\"Sintoma\")\n",
        "serie_symptom_cases_smoothed.plot(label=\"Sintoma Suav\")\n",
        "SERIE_PUBLICATION_CASES.plot(label=\"Publicação\")\n",
        "serie_publication_cases_smoothed.plot(label=\"Publicação Suav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3GVSE6_sYS2"
      },
      "source": [
        "#Escalar (normalização entre 0 e 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nqsfJ8_r87-"
      },
      "outputs": [],
      "source": [
        "def process_scaler(serie):\n",
        "  #Pré-processamento - scalar (normalizar entre 0 e 1)\n",
        "  Scaler_obj = Scaler()\n",
        "  serieTemp_scaled = Scaler_obj.fit_transform(serie)\n",
        "\n",
        "  return serieTemp_scaled, Scaler_obj\n",
        "  #Retorno: Objeto Scaler da série, objeto scaler da covariável, serie temporal já normalizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyjKyAQdAU1u"
      },
      "outputs": [],
      "source": [
        "#pré-processamento e normalização [0, 1]\n",
        "serie_symptom_cases_normalized, SCALER_symptom_cases = process_scaler(serie_symptom_cases_smoothed)\n",
        "serie_publication_cases_normalized, SCALER_publication_cases = process_scaler(serie_publication_cases_smoothed)\n",
        "\n",
        "serie_symptom_cases_7previousdays_normalized, SCALER_symptom_7days = process_scaler(SERIE_SYMPTOM_CASES_7PREVIOUSDAYS)\n",
        "serie_publication_cases_7previousdays_normalized, SCALER_publication_7days = process_scaler(SERIE_PUBLICATION_CASES_7PREVIOUSDAYS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU-dDqDAKf6Q"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 6))\n",
        "\n",
        "serie_symptom_cases_normalized.plot()\n",
        "serie_publication_cases_normalized.plot()\n",
        "\n",
        "serie_symptom_cases_7previousdays_normalized.plot()\n",
        "serie_publication_cases_7previousdays_normalized.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjTXoInTtBX-"
      },
      "source": [
        "#Definição de Covariáveis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiP3y4SdstLg"
      },
      "outputs": [],
      "source": [
        "def define_covariate_dates(serie_temp_normalizada):  #modificar nomes\n",
        "  #Definir covariavel ano\n",
        "  scaler_CovarYear, scaler_CovarMonth = Scaler(), Scaler()\n",
        "  covariate_y = datetime_attribute_timeseries(serie_temp_normalizada, attribute='year', add_length = FORECAST_DAYS)\n",
        "  covariate_y = scaler_CovarYear.fit_transform(covariate_y)\n",
        "\n",
        "  #Definir covariavel mes\n",
        "  covariate_m = datetime_attribute_timeseries(serie_temp_normalizada, attribute='month', add_length = FORECAST_DAYS)\n",
        "  covariate_m = scaler_CovarMonth.fit_transform(covariate_m)\n",
        "\n",
        "  #Definir covariavel day of week\n",
        "  covariate_dw = datetime_attribute_timeseries(serie_temp_normalizada, attribute='dayofweek', add_length = FORECAST_DAYS)\n",
        "  covariate_dw = scaler_CovarMonth.fit_transform(covariate_dw)\n",
        "\n",
        "  return covariate_y, covariate_m, covariate_dw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3MBajxnsxeI"
      },
      "outputs": [],
      "source": [
        "#definição das covariáveis\n",
        "covariate_year_symptom_cases, covariate_month_symptom_cases, covariate_dayweek_symptom_cases = define_covariate_dates(serie_symptom_cases_normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKQhjyZR1MyB"
      },
      "outputs": [],
      "source": [
        "#Visualização das series covariaveis de data\n",
        "plt.figure(figsize = (10, 6))\n",
        "covariate_year_symptom_cases.plot()\n",
        "covariate_month_symptom_cases.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJzs5rQ-AG6O"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 6))\n",
        "covariate_dayweek_symptom_cases[-30:].plot(marker='o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raNjZdiDtFo9"
      },
      "source": [
        "#Divisão do dataset de treino e validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHpCgDl0s5Z6"
      },
      "outputs": [],
      "source": [
        "#Dividir em dados de treino e teste\n",
        "def split_train_val_series(serie):\n",
        "  s_train, s_val = serie[:-VALIDATION_DAYS_SLICED], serie[-VALIDATION_DAYS_SLICED:]\n",
        "  \n",
        "  return s_train, s_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNaxpEp5s3xE"
      },
      "outputs": [],
      "source": [
        "#divisão dos dados de treino e testes\n",
        "serieTrain_symptom_cases, serieVal_symptom_cases = split_train_val_series(serie_symptom_cases_normalized)\n",
        "serieTrain_publication_cases, serieVal_publication_cases = split_train_val_series(serie_publication_cases_normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0mxNGzCGEzb"
      },
      "outputs": [],
      "source": [
        "#Visualização das Series Alvos\n",
        "plt.figure(figsize = (10, 6))\n",
        "\n",
        "serieTrain_symptom_cases[-30:].plot()   # plotagem dos ultimos 30 dias para treino\n",
        "serieVal_symptom_cases.plot()\n",
        "serieTrain_publication_cases[-30:].plot()   # plotagem dos ultimos 30 dias para treino\n",
        "serieVal_publication_cases.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZMrDZWIUhvn"
      },
      "source": [
        "#Treinamento Modelos: NBEATS, TCN, Transformer, TFT, NHITS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kAVZ579wmc6"
      },
      "source": [
        "##Preparação dos dados para treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaUw4qsNXEmj"
      },
      "outputs": [],
      "source": [
        "#Junção das variaveis alvo e coovariaveis\n",
        "series_Train = serieTrain_symptom_cases.stack(serieTrain_publication_cases)\n",
        "\n",
        "covariatesDate = covariate_year_symptom_cases.stack(covariate_month_symptom_cases.stack(covariate_dayweek_symptom_cases))\n",
        "covariatesCases = serie_symptom_cases_7previousdays_normalized.stack(serie_publication_cases_7previousdays_normalized)\n",
        "covariatesVacination = serie_vaccination_1Dose.stack(serie_vaccination_2Dose.stack(serie_vaccination_3Dose))\n",
        "\n",
        "#O [:-FORECAST_DAYS] é um corte na covariatesDate para que as covariaveis tenham o mesmo numero de dias.\n",
        "covariates = covariatesDate[:-FORECAST_DAYS].stack(covariatesCases.stack(covariatesVacination))\n",
        "\n",
        "plt.figure(figsize = (10, 6))\n",
        "series_Train.plot()\n",
        "covariates.plot()\n",
        "print(len(covariates))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWmsxN1HZEDu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 6))\n",
        "series_Train[-40:].plot()\n",
        "covariates[-40:].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GClb9V0DUx0O"
      },
      "source": [
        "##Acelerador COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga5MA841FBus"
      },
      "outputs": [],
      "source": [
        "# Utilizar o acelerador de GPU do colab (para treinamento)\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "  tf.device('/device:GPU:0')\n",
        "  print()\n",
        "\n",
        "  physical_devices = tf.config.experimental.list_logical_devices('GPU')\n",
        "  print(\"GPUs available: \", physical_devices)\n",
        "  print(\"GPUs available: \", len(physical_devices))\n",
        "  print(physical_devices[0].name)\n",
        "  print()\n",
        "\n",
        "  # Qual placa de video GPU estou usando?\n",
        "  from tensorflow.python.client import device_lib\n",
        "  device_lib.list_local_devices()\n",
        "  print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjx2Vl0wwuIZ"
      },
      "source": [
        "##Área de funções para treinar modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEeLsfBZrMAx"
      },
      "outputs": [],
      "source": [
        "#Parâmetros para auxiliar no desempenho computacional dos modelos,\n",
        "#e parâmetros que são comuns a todos os modelos.\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "stopper = EarlyStopping(\n",
        "    monitor=\"train_loss\",\n",
        "    patience=5,\n",
        "    #min_delta=0.05,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "pl_trainer_dic = {\n",
        "      \"callbacks\": [stopper]#,\n",
        "      #\"accelerator\": \"auto\"\n",
        "      #\"max_epochs\": 150,\n",
        "      #\"restore_best_weights\": True\n",
        "    }\n",
        "\n",
        "n_epochs_list = [30]\n",
        "batch_size_list = [38]#[16,32,64]\n",
        "force_reset_list = [True]\n",
        "dropout_list = [0.05, 0.1]\n",
        "likelihood_list = [\n",
        "    GaussianLikelihood(), \n",
        "    QuantileRegression(\n",
        "        quantiles = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
        "        )]\n",
        "\n",
        "start_day = int(round(len(serieTrain_symptom_cases)*0.05,0))\n",
        "\n",
        "stride_steps = FORECAST_DAYS * 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAxLhqaZRkdH"
      },
      "source": [
        "\n",
        "##NBEATS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFTKpto5U8Sv"
      },
      "outputs": [],
      "source": [
        "def gridsearch_NBEATS(gridComplextyModel: int, time_series, covariates, FORECAST_DAYS):\n",
        "  \n",
        "  name_model = \"NBEATS-Model_\" + time.strftime(\"%d_%m_%Y\", time.localtime()) \n",
        "\n",
        "  if gridComplextyModel == 0:\n",
        "    model = NBEATSModel(\n",
        "      input_chunk_length = WORKOUT_DAYS_SLICED,\n",
        "      output_chunk_length = VALIDATION_DAYS_SLICED,   \n",
        "      kwargs= [0,\"Relu\"],   \n",
        "      n_epochs=10,\n",
        "      likelihood = likelihood_list[1],# GaussianLikelihood(),\n",
        "      model_name = name_model)\n",
        "    \n",
        "  elif gridComplextyModel == 1:\n",
        "    parameters = {\n",
        "      \"input_chunk_length\":[WORKOUT_DAYS_SLICED],\n",
        "      \"output_chunk_length\":[VALIDATION_DAYS_SLICED],\n",
        "      \"n_epochs\":[4],\n",
        "      \"batch_size\":[32] ,                           \n",
        "      \"force_reset\":force_reset_list,                             \n",
        "      \"generic_architecture\":[True,False],\n",
        "      \"num_stacks\":[2],\n",
        "      \"num_layers\":[2],\n",
        "      \"num_blocks\":[1],\n",
        "      \"layer_widths\":[8],  \n",
        "      \"dropout\":dropout_list,\n",
        "      \"activation\":['ReLU'],\n",
        "      \"likelihood\":likelihood_list, \n",
        "      \"model_name\":[name_model],\n",
        "      \"pl_trainer_kwargs\":[pl_trainer_dic]} \n",
        "\n",
        "    model_tuple = NBEATSModel.gridsearch(\n",
        "        parameters = parameters, \n",
        "        series = time_series, \n",
        "        forecast_horizon = FORECAST_DAYS, \n",
        "        start = start_day, #Começa apartir do dia correspondente ao valor de 11% da serie # WORKOUT_DAYS_SLICED * 3,\n",
        "        past_covariates = covariates,\n",
        "        stride = FORECAST_DAYS,\n",
        "        metric = rmse, \n",
        "        verbose = True,\n",
        "        n_jobs = -1)\n",
        "      \n",
        "    model = model_tuple[0]\n",
        "\n",
        "  elif gridComplextyModel == 2:\n",
        "    parameters = {\n",
        "      \"input_chunk_length\":[WORKOUT_DAYS_SLICED],\n",
        "      \"output_chunk_length\":[VALIDATION_DAYS_SLICED],\n",
        "      \"n_epochs\":n_epochs_list,\n",
        "      \"batch_size\":batch_size_list,                           \n",
        "      \"force_reset\":force_reset_list,                             \n",
        "      \"generic_architecture\":[True],\n",
        "      \"num_stacks\":[3],\n",
        "      \"num_layers\":[2,3],\n",
        "      \"num_blocks\":[1,2],\n",
        "      \"layer_widths\":[128],  \n",
        "      \"dropout\":dropout_list,\n",
        "      \"activation\":['ReLU','Tanh','Sigmoid'],\n",
        "      \"likelihood\":likelihood_list, \n",
        "      \"model_name\":[name_model],\n",
        "      \"pl_trainer_kwargs\":[pl_trainer_dic]}\n",
        "\n",
        "    model_tuple = NBEATSModel.gridsearch(\n",
        "      parameters = parameters, \n",
        "      series = time_series, \n",
        "      forecast_horizon = FORECAST_DAYS, \n",
        "      start = start_day, #Começa apartir do dia correspondente ao valor de 11% da serie # WORKOUT_DAYS_SLICED * 3,\n",
        "      past_covariates = covariates,\n",
        "      stride = stride_steps,# 1\n",
        "      metric = rmse, \n",
        "      verbose = True,\n",
        "      n_jobs = -1)\n",
        "    \n",
        "    model = model_tuple[0]\n",
        "\n",
        "  else:\n",
        "    print('Valor de parâmetro indisponível')\n",
        "\n",
        "  model.fit(\n",
        "      series = time_series, \n",
        "      past_covariates = covariates, \n",
        "      verbose = False)\n",
        "  \n",
        "  return model, model_tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRlFXaEprRpu"
      },
      "source": [
        "\n",
        "##NHITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaSqFd0wzV5m"
      },
      "outputs": [],
      "source": [
        "def gridsearch_NHITS(gridComplextyModel: int, time_series, covariates, FORECAST_DAYS):\n",
        "  \n",
        "  name_model = \"NHITS-Model_\" + time.strftime(\"%d_%m_%Y\", time.localtime()) \n",
        "\n",
        "  if gridComplextyModel == 0:\n",
        "    model = NHiTSModel(\n",
        "      input_chunk_length = WORKOUT_DAYS_SLICED,\n",
        "      output_chunk_length = VALIDATION_DAYS_SLICED,      \n",
        "      n_epochs=10,\n",
        "      kwargs= [0,\"Relu\"],\n",
        "      likelihood = likelihood_list[1],# GaussianLikelihood(),\n",
        "      model_name = name_model)\n",
        "\n",
        "  elif gridComplextyModel == 1:\n",
        "    parameters = {\n",
        "      \"input_chunk_length\":[WORKOUT_DAYS_SLICED],\n",
        "      \"output_chunk_length\":[VALIDATION_DAYS_SLICED],\n",
        "      \"n_epochs\":[4],\n",
        "      \"batch_size\":[32],                           \n",
        "      \"force_reset\": force_reset_list,                             \n",
        "      \"num_stacks\":[2],\n",
        "      \"num_layers\":[1],\n",
        "      \"num_blocks\":[1],\n",
        "      \"layer_widths\":[8],\n",
        "      \"dropout\":[0],\n",
        "      \"activation\":['ReLU'],  \n",
        "      \"likelihood\": likelihood_list, \n",
        "      \"model_name\": [name_model],      \n",
        "      #\"pl_trainer_kwargs\":[pl_trainer_dic]\n",
        "      }\n",
        "\n",
        "    model_tuple = NHiTSModel.gridsearch(\n",
        "      parameters = parameters, \n",
        "      series = time_series, \n",
        "      forecast_horizon = FORECAST_DAYS, \n",
        "      start = start_day, #Começa apartir do dia correspondente ao valor de 11% da serie # WORKOUT_DAYS_SLICED * 3,\n",
        "      past_covariates = covariates,\n",
        "      stride = FORECAST_DAYS,\n",
        "      metric = rmse, \n",
        "      verbose = False,\n",
        "      n_jobs = 1\n",
        "      )\n",
        "  \n",
        "    model = model_tuple[0]    \n",
        "\n",
        "  elif gridComplextyModel == 2:\n",
        "    parameters = {\n",
        "      \"input_chunk_length\":[WORKOUT_DAYS_SLICED],\n",
        "      \"output_chunk_length\":[VALIDATION_DAYS_SLICED],\n",
        "      \"n_epochs\":n_epochs_list,\n",
        "      \"batch_size\":batch_size_list,                           \n",
        "      \"force_reset\":force_reset_list,                             \n",
        "      \"num_stacks\":[3],\n",
        "      \"num_layers\":[2,3],\n",
        "      \"num_blocks\":[1,2],\n",
        "      \"layer_widths\":[128],\n",
        "      \"dropout\": dropout_list,\n",
        "      \"activation\":['ReLU','Tanh','Sigmoid'],      \n",
        "      \"likelihood\":likelihood_list, \n",
        "      \"model_name\":[name_model],      \n",
        "      \"pl_trainer_kwargs\":[pl_trainer_dic]\n",
        "    }\n",
        "\n",
        "    model_tuple = NHiTSModel.gridsearch(\n",
        "      parameters = parameters, \n",
        "      series = time_series, \n",
        "      forecast_horizon = FORECAST_DAYS, \n",
        "      start = start_day, #Começa apartir do dia correspondente ao valor de 11% da serie # WORKOUT_DAYS_SLICED * 3, \n",
        "      past_covariates = covariates,\n",
        "      stride = stride_steps,# 1\n",
        "      metric = rmse, \n",
        "      verbose = True,\n",
        "      n_jobs = -1\n",
        "      )\n",
        "  \n",
        "    model = model_tuple[0]\n",
        "\n",
        "  else:\n",
        "    print('Valor de parâmetro indisponível')\n",
        "      \n",
        "  model.fit(\n",
        "      series = time_series, \n",
        "      past_covariates = covariates, \n",
        "      verbose = False)\n",
        "  \n",
        "  return model, model_tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dsg50dp3rQt8"
      },
      "source": [
        "\n",
        "##TCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jko9Y2n7rukV"
      },
      "outputs": [],
      "source": [
        "def gridsearch_TCN(gridComplextyModel: int, time_series, covariates, FORECAST_DAYS):\n",
        "  \n",
        "  name_model = \"TCN-Model_\" + time.strftime(\"%d_%m_%Y\", time.localtime()) \n",
        "\n",
        "  if gridComplextyModel == 0:\n",
        "    model = TCNModel(\n",
        "      input_chunk_length = WORKOUT_DAYS_SLICED,\n",
        "      output_chunk_length = VALIDATION_DAYS_SLICED,\n",
        "      n_epochs=10,\n",
        "      dropout=0.1,\n",
        "      dilation_base=2,\n",
        "      weight_norm=True,\n",
        "      kernel_size=5,\n",
        "      num_filters=3,\n",
        "      likelihood = likelihood_list[1],#GaussianLikelihood(),\n",
        "      model_name = name_model\n",
        "      )\n",
        "\n",
        "  elif gridComplextyModel == 1:\n",
        "    parameters = {\n",
        "      \"input_chunk_length\":[WORKOUT_DAYS_SLICED],\n",
        "      \"output_chunk_length\":[VALIDATION_DAYS_SLICED],\n",
        "      \"n_epochs\":[4],\n",
        "      \"batch_size\":[32],  \n",
        "      \"force_reset\":[True],\n",
        "      \"likelihood\":likelihood_list,\n",
        "      \"kernel_size\":[2],\n",
        "      \"num_layers\":[1],\n",
        "      \"num_filters\":[2],  \n",
        "      \"dilation_base\":[2],\n",
        "      \"weight_norm\":[False], \n",
        "      \"dropout\":[0],  \n",
        "      \"model_name\":[name_model],\n",
        "      #\"pl_trainer_kwargs\":[pl_trainer_dic]\n",
        "    }\n",
        "\n",
        "    model_tuple = TCNModel.gridsearch(\n",
        "        parameters = parameters,\n",
        "        series = time_series, \n",
        "        past_covariates = covariates,\n",
        "        forecast_horizon = FORECAST_DAYS, \n",
        "        stride = FORECAST_DAYS,# 1 \n",
        "        start = start_day, #Começa apartir do dia correspondente ao valor de 11% da serie # WORKOUT_DAYS_SLICED * 3,\n",
        "        metric = rmse, \n",
        "        verbose = True,\n",
        "        n_jobs = -1\n",
        "      )\n",
        "      \n",
        "    model = model_tuple[0]\n",
        "\n",
        "  elif gridComplextyModel == 2:\n",
        "    parameters = {\n",
        "      \"input_chunk_length\":[WORKOUT_DAYS_SLICED],\n",
        "      \"output_chunk_length\":[VALIDATION_DAYS_SLICED],\n",
        "      \"n_epochs\":n_epochs_list,\n",
        "      \"batch_size\":batch_size_list,  \n",
        "      \"force_reset\":force_reset_list,\n",
        "      \"likelihood\":likelihood_list,\n",
        "      \"kernel_size\":[3,4],\n",
        "      \"num_layers\":[None,1,2],\n",
        "      \"num_filters\":[2,3,4],  \n",
        "      \"dilation_base\":[2],\n",
        "      \"weight_norm\":[True,False],  \n",
        "      \"dropout\":dropout_list,  \n",
        "      \"model_name\":[name_model],\n",
        "      \"pl_trainer_kwargs\":[pl_trainer_dic]\n",
        "    }\n",
        "\n",
        "    model_tuple = TCNModel.gridsearch(\n",
        "      parameters = parameters,\n",
        "      series = time_series, \n",
        "      past_covariates = covariates,\n",
        "      forecast_horizon = FORECAST_DAYS, \n",
        "      stride = stride_steps,# 1 \n",
        "      start = start_day, \n",
        "      metric = rmse, \n",
        "      verbose = True,\n",
        "      n_jobs = -1\n",
        "    )\n",
        "  \n",
        "    model = model_tuple[0]\n",
        "\n",
        "  else:\n",
        "    print('Valor de parâmetro indisponível')\n",
        "  \n",
        "  model.fit(series = time_series, \n",
        "      past_covariates = covariates,\n",
        "      verbose = False)\n",
        "  \n",
        "  return model, model_tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlAnJTcSRf4T"
      },
      "source": [
        "##TFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtV3lr7wruN7"
      },
      "outputs": [],
      "source": [
        "def gridsearch_TFT(gridComplextyModel: int, time_series, covariates, FORECAST_DAYS):    \n",
        "\n",
        "  name_model = \"TFT-Model_\" + time.strftime(\"%d_%m_%Y\", time.localtime())\n",
        "\n",
        "  if gridComplextyModel == 0:\n",
        "    model = TFTModel(\n",
        "      input_chunk_length = WORKOUT_DAYS_SLICED,\n",
        "      output_chunk_length = VALIDATION_DAYS_SLICED,\n",
        "      hidden_size=4,\n",
        "      lstm_layers=1,\n",
        "      num_attention_heads=4,\n",
        "      dropout=0.1,\n",
        "      batch_size=2,\n",
        "      n_epochs=4,\n",
        "      add_relative_index = True,\n",
        "      add_encoders = None, \n",
        "      likelihood = likelihood_list[1],\n",
        "      model_name = name_model\n",
        "    )\n",
        "\n",
        "  elif gridComplextyModel == 1:\n",
        "    parameters = {\n",
        "      \"input_chunk_length\":[WORKOUT_DAYS_SLICED],\n",
        "      \"output_chunk_length\":[VALIDATION_DAYS_SLICED],\n",
        "      \"n_epochs\":[4],\n",
        "      \"batch_size\":[32],  \n",
        "      \"force_reset\": force_reset_list,\n",
        "      \"likelihood\": likelihood_list,\n",
        "      \"hidden_size\": [1],\n",
        "      \"lstm_layers\": [1], \n",
        "      \"num_attention_heads\": [2], \n",
        "      \"full_attention\": [False], \n",
        "      \"hidden_continuous_size\": [2], \n",
        "      \"add_relative_index\": [True],  \n",
        "      \"dropout\":[0], \n",
        "      #\"pl_trainer_kwargs\":[pl_trainer_dic], \n",
        "      \"model_name\":[name_model]  \n",
        "    }\n",
        "\n",
        "    model_tuple = TFTModel.gridsearch(\n",
        "      parameters = parameters,\n",
        "      series = time_series, \n",
        "      past_covariates = covariates,\n",
        "      forecast_horizon = FORECAST_DAYS, \n",
        "      stride = FORECAST_DAYS,\n",
        "      start = start_day, #Começa apartir do dia correspondente ao valor de 11% da serie # WORKOUT_DAYS_SLICED * 3,\n",
        "      metric = rmse, \n",
        "      verbose = True,\n",
        "      n_jobs = -1\n",
        "    )\n",
        "  \n",
        "    model = model_tuple[0]\n",
        "\n",
        "  elif gridComplextyModel == 2:\n",
        "    parameters = {\n",
        "      \"input_chunk_length\":[WORKOUT_DAYS_SLICED],\n",
        "      \"output_chunk_length\":[VALIDATION_DAYS_SLICED],\n",
        "      \"n_epochs\":n_epochs_list,\n",
        "      \"batch_size\":batch_size_list,  \n",
        "      \"force_reset\":force_reset_list,\n",
        "      \"likelihood\":likelihood_list,\n",
        "      \"hidden_size\":[8],\n",
        "      \"lstm_layers\":[1,2,3], \n",
        "      \"num_attention_heads\":[3,4,5], \n",
        "      \"full_attention\":[True], \n",
        "      \"hidden_continuous_size\":[8], \n",
        "      \"add_relative_index\":[True],  \n",
        "      \"dropout\":dropout_list, \n",
        "      \"pl_trainer_kwargs\":[pl_trainer_dic], \n",
        "      \"model_name\":[name_model]  \n",
        "    }\n",
        "\n",
        "    #WORKOUT_DAYS_SLICED * 3\n",
        "    model_tuple = TFTModel.gridsearch(\n",
        "      parameters = parameters,\n",
        "      series = time_series, \n",
        "      past_covariates = covariates,\n",
        "      forecast_horizon = FORECAST_DAYS, \n",
        "      stride = stride_steps,# 1 \n",
        "      start = start_day, #Começa apartir do dia correspondente ao valor de 11% da serie # WORKOUT_DAYS_SLICED * 3,\n",
        "      metric = rmse, \n",
        "      verbose = True,\n",
        "      n_jobs = -1\n",
        "    )\n",
        "  \n",
        "    model = model_tuple[0]\n",
        "\n",
        "  else:\n",
        "    print('Valor de parâmetro indisponível')\n",
        "\n",
        "  model.fit(series = time_series, \n",
        "            past_covariates = covariates, \n",
        "            verbose = False)\n",
        "  \n",
        "  return model, model_tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Qij5KadExI"
      },
      "source": [
        "##Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_T6gZnUHRUL"
      },
      "outputs": [],
      "source": [
        "def gridsearch_TRANSFORMER(gridComplextyModel: int, time_series, covariates,FORECAST_DAYS):\n",
        "  \n",
        "  name_model = \"TRANSFORMER-Model_\" + time.strftime(\"%d_%m_%Y\", time.localtime())\n",
        "\n",
        "  if gridComplextyModel == 0:\n",
        "    model = TransformerModel(\n",
        "      input_chunk_length = WORKOUT_DAYS_SLICED,\n",
        "      output_chunk_length = VALIDATION_DAYS_SLICED,\n",
        "      batch_size=32,\n",
        "      n_epochs=10,\n",
        "      model_name = name_model,    \n",
        "      d_model=4,\n",
        "      nhead=2,\n",
        "      num_encoder_layers=2,\n",
        "      num_decoder_layers=2,\n",
        "      dim_feedforward=128,\n",
        "      dropout=0,\n",
        "      activation=\"relu\",\n",
        "      force_reset=True,\n",
        "      likelihood = likelihood_list[1])\n",
        "\n",
        "  elif gridComplextyModel == 1:\n",
        "    parameters = {\n",
        "      \"input_chunk_length\":[WORKOUT_DAYS_SLICED],\n",
        "      \"output_chunk_length\":[VALIDATION_DAYS_SLICED],\n",
        "      \"n_epochs\":[4],\n",
        "      \"batch_size\":[32],  \n",
        "      \"force_reset\":force_reset_list,\n",
        "      \"likelihood\":likelihood_list,\n",
        "      \"d_model\":[2], \n",
        "      \"nhead\":[2], \n",
        "      \"num_encoder_layers\":[2], \n",
        "      \"num_decoder_layers\":[2], \n",
        "      \"dim_feedforward\":[2],\n",
        "      \"activation\":['relu'],   \n",
        "      \"dropout\":[0],\n",
        "      #\"pl_trainer_kwargs\":[pl_trainer_dic],\n",
        "      \"model_name\":[name_model]  \n",
        "    }\n",
        "\n",
        "    model_tuple = TransformerModel.gridsearch(\n",
        "      parameters = parameters,\n",
        "      series = time_series, \n",
        "      past_covariates = covariates,\n",
        "      forecast_horizon = FORECAST_DAYS, \n",
        "      stride = FORECAST_DAYS,# 1 \n",
        "      start = start_day, #Começa apartir do dia correspondente ao valor de 11% da serie # WORKOUT_DAYS_SLICED * 3,\n",
        "      metric = rmse, \n",
        "      verbose = True,\n",
        "      n_jobs = -1\n",
        "    )\n",
        "\n",
        "    model = model_tuple[0]\n",
        "\n",
        "  elif gridComplextyModel == 2:\n",
        "    parameters = {\n",
        "      \"input_chunk_length\":[WORKOUT_DAYS_SLICED],\n",
        "      \"output_chunk_length\":[VALIDATION_DAYS_SLICED],\n",
        "      \"n_epochs\":n_epochs_list,\n",
        "      \"batch_size\":batch_size_list,  \n",
        "      \"force_reset\":force_reset_list,\n",
        "      \"likelihood\":likelihood_list,\n",
        "      \"d_model\":[32], \n",
        "      \"nhead\":[2,4], \n",
        "      \"num_encoder_layers\":[2,3,4], \n",
        "      \"num_decoder_layers\":[2,3,4], \n",
        "      \"dim_feedforward\":[256],\n",
        "      \"activation\":['relu','gelu'],   \n",
        "      \"dropout\":dropout_list,\n",
        "      \"pl_trainer_kwargs\":[pl_trainer_dic],\n",
        "      \"model_name\":[name_model]  \n",
        "      }\n",
        "\n",
        "    model_tuple = TransformerModel.gridsearch(\n",
        "      parameters = parameters,\n",
        "      series = time_series, \n",
        "      past_covariates = covariates,\n",
        "      forecast_horizon = FORECAST_DAYS, \n",
        "      stride = stride_steps,# 1 \n",
        "      start = start_day, #Começa apartir do dia correspondente ao valor de 11% da serie # WORKOUT_DAYS_SLICED * 3,\n",
        "      metric = rmse, \n",
        "      verbose = True,\n",
        "      n_jobs = -1\n",
        "    )\n",
        "\n",
        "    model = model_tuple[0]\n",
        "\n",
        "  else:\n",
        "    print('Valor de parâmetro indisponível')\n",
        "\n",
        "  model.fit(series = time_series, \n",
        "            past_covariates = covariates, \n",
        "            verbose=False)\n",
        "  \n",
        "  return model, model_tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CIaFl_kwrXW"
      },
      "source": [
        "##Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i70ibCabsktN"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "import datetime\n",
        "start_T = timer()\n",
        "hora_atual = datetime.datetime.now()\n",
        "print('Hora de inicialiação casos: ')\n",
        "print(hora_atual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "075wYDuNXFuu"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "#model_NBEATSModel, gridValues_NBEATSModel = gridsearch_NBEATS(1, series_Train, covariates, FORECAST_DAYS)\n",
        "end = timer()\n",
        "print(\"Tempo decorrido de treinamento: {:.2f} minutos\".format((end - start)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRZggrnXtBH-"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "#model_TCNModel, gridValues_TCNModel = gridsearch_TCN(1, series_Train, covariates, FORECAST_DAYS)\n",
        "end = timer()\n",
        "print(\"Tempo decorrido de treinamento: {:.2f} minutos\".format((end - start)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGCM13s6tFu6"
      },
      "outputs": [],
      "source": [
        "#start = timer()\n",
        "#model_TFTModel, gridValues_TFTModel = gridsearch_TFT(1, series_Train, covariates, FORECAST_DAYS)\n",
        "end = timer()\n",
        "print(\"Tempo decorrido de treinamento: {:.2f} minutos\".format((end - start)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6UtSpjwHrVE"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "#model_TRANSFORMERModel, gridValues_TRANSFORMERModel = gridsearch_TRANSFORMER(1, series_Train, covariates, FORECAST_DAYS)\n",
        "end = timer()\n",
        "print(\"Tempo decorrido de treinamento: {:.2f} minutos\".format((end - start)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJjJh60R1cVs"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "model_NHITSModel, gridValues_NHITSModel = gridsearch_NHITS(1, series_Train, covariates, FORECAST_DAYS)\n",
        "end = timer()\n",
        "print(\"Tempo decorrido de treinamento: {:.2f} minutos\".format((end - start)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSQUc4DxslZF"
      },
      "outputs": [],
      "source": [
        "end_T = timer()\n",
        "print(\"Tempo decorrido de treinamento total: {:.2f} minutos\".format((end_T - start_T)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYEF9NrpaKVc"
      },
      "outputs": [],
      "source": [
        "hora_atual = datetime.datetime.now()\n",
        "print('Hora de finalização casos: ') \n",
        "print(hora_atual)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRGCroDjFQaq"
      },
      "source": [
        "#Exportação dos modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmU6dvrwFr7S"
      },
      "outputs": [],
      "source": [
        "def saving_templates(model, path_model):\n",
        "  model.save_model(path_model + model.model_name + '.pth.tar')\n",
        "  print(\"Caminho (pasta) do Modelo: \", path_model + model.model_name + '.pth.tar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrBbpoBZmEll"
      },
      "outputs": [],
      "source": [
        "#saving_templates(model_NBEATSModel, PATH_MODELS)\n",
        "#print('NBEATS EXPORT')\n",
        "\n",
        "#saving_templates(model_TCNModel, PATH_MODELS)\n",
        "#print('TCN EXPORT')\n",
        "\n",
        "#saving_templates(model_TFTModel, PATH_MODELS)\n",
        "#print('TFT EXPORT')\n",
        "\n",
        "#saving_templates(model_TRANSFORMERModel, PATH_MODELS)\n",
        "#print('TRNSFORMER EXPORT')\n",
        "\n",
        "saving_templates(model_NHITSModel, PATH_MODELS)\n",
        "print('NHITS EXPORT')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m63m8kv7s9X-"
      },
      "source": [
        "#Salvar parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ra7IKMHs9X_"
      },
      "outputs": [],
      "source": [
        "def save_parameters(path_params, model, grid):\n",
        "  arquivo = open(path_params + 'BestParameters_' + model.model_name + '_param.txt', 'w')\n",
        "  string = str(grid)\n",
        "  arquivo.write(string)\n",
        "  arquivo.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mcp0sX7s9YB"
      },
      "outputs": [],
      "source": [
        "save_parameters(PATH_PARAMS, model_NBEATSModel, gridValues_NBEATSModel[1])\n",
        "\n",
        "save_parameters(PATH_PARAMS, model_TCNModel, gridValues_TCNModel[1])\n",
        "\n",
        "save_parameters(PATH_PARAMS, model_TFTModel, gridValues_TFTModel[1])\n",
        "\n",
        "save_parameters(PATH_PARAMS, model_TRANSFORMERModel, gridValues_TRANSFORMERModel[1])\n",
        "\n",
        "save_parameters(PATH_PARAMS, model_NHITSModel, gridValues_NHITSModel[1])\n",
        "\n",
        "print('/n/nParametros salvos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amugWa5hqp26"
      },
      "outputs": [],
      "source": [
        "#%run \"/content/drive/My Drive/NPCA - COVID/Colab/TREINO E PREDIÇÃO/TREINAR_MODELOS_covid_obitos_ver1.ipynb\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RvpQPdBdUian",
        "I3kwbr4cUrHH",
        "RiNGBVhbu12n",
        "8xX7CbOt0G9_",
        "wlp3EKa5PTP6",
        "eCSPZIK4DriJ",
        "kZlwXrseqluW",
        "y3GVSE6_sYS2",
        "mjTXoInTtBX-",
        "raNjZdiDtFo9",
        "hjx2Vl0wwuIZ",
        "lAxLhqaZRkdH",
        "fRlFXaEprRpu",
        "Dsg50dp3rQt8",
        "DlAnJTcSRf4T",
        "85Qij5KadExI"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}